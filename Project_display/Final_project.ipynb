{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get 10000 images from ccpd_base file, and split it into train set, test set and validation set.(7:2:1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import shutil\n",
    "from shutil import copy2\n",
    "# trainfiles = os.listdir(r\"D:\\UW\\EE567-project\\CCPD2019\\ccpd_base\")  #（图片文件夹）\n",
    "# # num_train = len(trainfiles)\n",
    "# num_train = 10000\n",
    "# print(\"num_train: \" + str(num_train) )\n",
    "# index_list = list(range(num_train))\n",
    "# print(index_list)\n",
    "# random.shuffle(index_list)  # 打乱顺序\n",
    "# num = 0\n",
    "# trainDir = r\"D:\\UW\\EE567-project\\train\"   #（将图片文件夹中的6份放在这个文件夹下）\n",
    "# validDir = r\"D:\\UW\\EE567-project\\val\"     #（将图片文件夹中的2份放在这个文件夹下）\n",
    "# testDir = r\"D:\\UW\\EE567-project\\test\"   #（将图片文件夹中的2份放在这个文件夹下）\n",
    "# for i in index_list:\n",
    "#     fileName = os.path.join(r\"D:\\UW\\EE567-project\\CCPD2019\\ccpd_base\", trainfiles[i])  #（图片文件夹）+图片名=图片地址\n",
    "#     if num < num_train*0.7:  # 7:1:2\n",
    "#         print(str(fileName))\n",
    "#         copy2(fileName, trainDir)\n",
    "#     elif num < num_train*0.8:\n",
    "#         print(str(fileName))\n",
    "#         copy2(fileName, validDir)\n",
    "#     else:\n",
    "#         print(str(fileName))\n",
    "#         copy2(fileName, testDir)\n",
    "#     num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect license plates from images and save detected license plates into `./dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# 参考 https://blog.csdn.net/qq_36516958/article/details/114274778\n",
    "# https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data#2-create-labels\n",
    "from PIL import Image\n",
    "\n",
    "test_data_path = r'D:\\UW\\EE567-project\\test'\n",
    "train_data_path = r'D:\\UW\\EE567-project\\train'\n",
    "val_data_path = r'D:\\UW\\EE567-project\\val'\n",
    "\n",
    "test_path = r\"D:\\UW\\EE567-project\\dataset\\test\"\n",
    "train_path = r\"D:\\UW\\EE567-project\\dataset\\train\"\n",
    "val_path = r\"D:\\UW\\EE567-project\\dataset\\val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# provinces = [\"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\", \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\", \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\", \"警\", \"学\", \"O\"]\n",
    "# alphabets = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W',\n",
    "#              'X', 'Y', 'Z', 'O']\n",
    "# ads = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X',\n",
    "#        'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'O']\n",
    "# num = 0\n",
    "# def make_dataset(input_path, output_path):\n",
    "#     for filename in os.listdir(input_path):\n",
    "#         global num\n",
    "#         num += 1\n",
    "#         result = \"\"\n",
    "#         _, _, box, points, plate, brightness, blurriness = filename.split('-')\n",
    "#         list_plate = plate.split('_')  # 读取车牌\n",
    "#         result += provinces[int(list_plate[0])]\n",
    "#         result += alphabets[int(list_plate[1])]\n",
    "#         result += ads[int(list_plate[2])] + ads[int(list_plate[3])] + ads[int(list_plate[4])] + ads[int(list_plate[5])] + ads[int(list_plate[6])]\n",
    "#         print(result)\n",
    "#         img_path = os.path.join(input_path, filename)\n",
    "#         img = cv2.imread(img_path)\n",
    "#         assert os.path.exists(img_path), \"image file {} dose not exist.\".format(img_path)\n",
    "\n",
    "#         box = box.split('_')  # 车牌边界\n",
    "#         box = [list(map(int, i.split('&'))) for i in box]\n",
    "\n",
    "#         xmin = box[0][0]\n",
    "#         xmax = box[1][0]\n",
    "#         ymin = box[0][1]\n",
    "#         ymax = box[1][1]\n",
    "\n",
    "#         img = Image.fromarray(img)\n",
    "#         img = img.crop((xmin, ymin, xmax, ymax))  # 裁剪出车牌位置\n",
    "#         img = img.resize((94, 24), Image.LANCZOS)\n",
    "#         img = np.asarray(img)  # 转成array,变成24*94*3\n",
    "\n",
    "#         output_file = os.path.join(output_path, \"{}.jpg\".format(result))\n",
    "#         cv2.imencode('.jpg', img)[1].tofile(output_file)\n",
    "#         # 图片中文名会报错\n",
    "#         # cv2.imwrite(r\"K:\\MyProject\\datasets\\ccpd\\new\\ccpd_2020\\rec_images\\train\\{}.jpg\".format(result), img)  # 改成自己存放的路径\n",
    "# make_dataset(test_data_path, test_path)\n",
    "# make_dataset(train_data_path, train_path)\n",
    "# make_dataset(val_data_path, val_path)\n",
    "# print(\"Total number of images:{}\".format(num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "CHARS = ['京', '沪', '津', '渝', '冀', '晋', '蒙', '辽', '吉', '黑',\n",
    "         '苏', '浙', '皖', '闽', '赣', '鲁', '豫', '鄂', '湘', '粤',\n",
    "         '桂', '琼', '川', '贵', '云', '藏', '陕', '甘', '青', '宁',\n",
    "         '新',\n",
    "         '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "         'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K',\n",
    "         'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V',\n",
    "         'W', 'X', 'Y', 'Z', 'I', 'O', '-'\n",
    "         ]\n",
    "CHARS_DICT = {char:i for i, char in enumerate(CHARS)}\n",
    "\n",
    "def list_images(directory):\n",
    "    \"\"\"\n",
    "    替代imutils.paths.list_images函数\n",
    "    递归查找目录中的所有图像文件\n",
    "    \"\"\"\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff']\n",
    "    image_paths = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if any(file.lower().endswith(ext) for ext in image_extensions):\n",
    "                image_paths.append(os.path.join(root, file))\n",
    "    \n",
    "    return image_paths\n",
    "\n",
    "class LPRDataLoader(Dataset):\n",
    "    def __init__(self, img_dir, imgSize, lpr_max_len, PreprocFun=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.img_paths = []\n",
    "        for i in range(len(img_dir)):\n",
    "            self.img_paths += list_images(img_dir[i])\n",
    "        random.shuffle(self.img_paths)\n",
    "        self.img_size = imgSize\n",
    "        self.lpr_max_len = lpr_max_len\n",
    "        if PreprocFun is not None:\n",
    "            self.PreprocFun = PreprocFun\n",
    "        else:\n",
    "            self.PreprocFun = self.transform\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        filename = self.img_paths[index]\n",
    "        # Image = cv2.imread(filename)\n",
    "        image = cv2.imdecode(np.fromfile(filename, dtype=np.uint8), -1)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        height, width, _ = image.shape\n",
    "        if height != self.img_size[1] or width != self.img_size[0]:\n",
    "            image = cv2.resize(image, self.img_size)\n",
    "        image = self.PreprocFun(image)\n",
    "        \n",
    "        basename = os.path.basename(filename)\n",
    "        imgname, suffix = os.path.splitext(basename)\n",
    "        imgname = imgname.split(\"-\")[0].split(\"_\")[0]\n",
    "        \n",
    "        label = list()\n",
    "        for c in imgname:\n",
    "            # one_hot_base = np.zeros(len(CHARS))\n",
    "            # one_hot_base[CHARS_DICT[c]] = 1\n",
    "            label.append(CHARS_DICT[c])\n",
    "            \n",
    "        if len(label) == 8:\n",
    "            if self.check(label) == False:\n",
    "                print(imgname)\n",
    "                assert 0, \"Error label ^~^!!!\"\n",
    "                \n",
    "        return image, label, len(label)\n",
    "    \n",
    "    def transform(self, img):\n",
    "        img = img.astype('float32')\n",
    "        img -= 127.5\n",
    "        img *= 0.0078125\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "        return img\n",
    "    \n",
    "    def check(self, label):\n",
    "        if label[2] != CHARS_DICT['D'] and label[2] != CHARS_DICT['F'] \\\n",
    "                and label[-1] != CHARS_DICT['D'] and label[-1] != CHARS_DICT['F']:\n",
    "            print(\"Error label, Please check!\")\n",
    "            return False\n",
    "        else:\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAYAF4DASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDj7Tw69xD50SM6njrU114eFoQkucnoM12Hh0iPQQHJBZzn88VFrNqq3scI+djjH1ry51JdGe1RpQqO3KjnP+ETktbWG/dA0cowEPaq0ei/Pjd944GBXpMsJmtLqxWHIhiTYSOp71gxxIzW5UYl80btv3QM1nVxFSLUWbYWhSnSlUS2MiPwkqz/AGaS4U3JwfK3ZIzTB4bT7LezPkSWzKu0d8nFdXcwqPFwHJJlTn1GOtTQ+VGutmeIyRJNyAeeaI1JSVyPYRi4uKumcbH4aWXS5r3kFJAgXP61oXHhnTrZ0immcyNGH244rcdoJfDNy1vEUVJFzk55zRJe3aSWqTWKlXCoGA3EjNUpt7MfsY3+HTUwLPQkvppLKBttosZd2xzj61Yg8PadJYzy2cjM0C7mB7itW4b+x9buBbIphACtGTwc9RVi5m+x6M/+hpbz3mFC5yxXuTU+06NlugoTU4x+KxgppOnwadDeXqyA3DkKi84A71meItGt7I20ttkx3Cbxk4IFdNp73EohsRCk4Un5n6IOprK8VyR3mrIkTqIYY9gJHBOecCtKdpa3OfGRnC6aMa18RyWMSRHBiBzg96nm8YvcXdvN9lXdGRu+b72P5UUV2fV4Hn/WaiejL8PivUDLJeSlGLkgQhgAB2rGi8Qs7sWwnltkYoooeHpyauEMXVpxtB2uatn4xup5GZoreORVJWRsk/hVWy1+cW9/FJIixzuCzEcmiiq9lHaxVOrNLRhYeIJzp81nMQsbuX5HLY9+1W7PxZfC0zI6eYg/dnGSPSiihUoLZFSrVJKzkZEGsX814zzyqSX3NuX3zVjU9UvLm7e5N2ZN2OCOR9BRRS9jDsCqzve5FBr93bRtFG5VX4Yg8moJb5ZMZDHFFFVGnFbIUpykndn/2Q==",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=94x24>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: ['皖', 'A', '0', 'H', '5', '9', '9']\n",
      "Length: 7\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "test_dataset = LPRDataLoader(test_path.split(','), (94, 24), 8)\n",
    "\n",
    "img, label, length = test_dataset[1000]\n",
    "image = np.transpose(img, (1, 2, 0)) / 0.0078125 + 127.5\n",
    "image = image.astype(np.uint8)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "pil_image = Image.fromarray(image)\n",
    "display(pil_image)\n",
    "print(\"Label:\", [CHARS[i] for i in label])\n",
    "print(\"Length:\", length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LPRNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class small_basic_block(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(small_basic_block, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out // 4, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(ch_out // 4, ch_out // 4, kernel_size=(3, 1), padding=(1, 0)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(ch_out // 4, ch_out // 4, kernel_size=(1, 3), padding=(0, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(ch_out // 4, ch_out, kernel_size=1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class LPRNet(nn.Module):\n",
    "    def __init__(self, lpr_max_len, class_num, dropout_rate):\n",
    "        super(LPRNet, self).__init__()\n",
    "        self.lpr_max_len = lpr_max_len\n",
    "        self.class_num = class_num\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm2d(num_features=64),\n",
    "            nn.ReLU(),\n",
    "            # 模拟MaxPool3d(1,3,3)的行为，但使用MaxPool2d\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            small_basic_block(ch_in=64, ch_out=128),\n",
    "            nn.BatchNorm2d(num_features=128),\n",
    "            nn.ReLU(),\n",
    "            # 模拟MaxPool3d(1,3,3)的行为，使用stride=(2,1,2)\n",
    "            nn.MaxPool2d(kernel_size=3, stride=(1, 2), padding=1),\n",
    "            small_basic_block(ch_in=128, ch_out=256),  # 修复：输入通道为128\n",
    "            nn.BatchNorm2d(num_features=256),\n",
    "            nn.ReLU(),  # 10\n",
    "            small_basic_block(ch_in=256, ch_out=256),\n",
    "            nn.BatchNorm2d(num_features=256),\n",
    "            nn.ReLU(),\n",
    "            # 模拟MaxPool3d的行为\n",
    "            nn.MaxPool2d(kernel_size=3, stride=(1, 2), padding=1),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(1, 4), stride=1),  # 修复：输入通道为256\n",
    "            nn.BatchNorm2d(num_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Conv2d(in_channels=256, out_channels=class_num, kernel_size=(13, 1), stride=1),\n",
    "            nn.BatchNorm2d(num_features=class_num),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.container = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=448+self.class_num, out_channels=self.class_num, kernel_size=(1, 1), stride=(1, 1)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        keep_features = list()\n",
    "        for i, layer in enumerate(self.backbone.children()):\n",
    "            x = layer(x)\n",
    "            if i in [2, 6, 13, 22]:  \n",
    "                keep_features.append(x)\n",
    "\n",
    "        global_context = list()\n",
    "        for i, f in enumerate(keep_features):\n",
    "            # 打印每个特征的形状以便调试\n",
    "            # print(f\"Feature {i} shape before pooling: {f.shape}\")\n",
    "            \n",
    "            if i in [0, 1]:\n",
    "                f = nn.AvgPool2d(kernel_size=5, stride=5)(f)\n",
    "            if i in [2]:\n",
    "                f = nn.AvgPool2d(kernel_size=(4, 10), stride=(4, 2))(f)\n",
    "            \n",
    "            # 记录池化后形状\n",
    "            # print(f\"Feature {i} shape after pooling: {f.shape}\")\n",
    "            \n",
    "            # 确保所有特征具有相同的空间维度 (H,W)\n",
    "            # 获取所有特征的最小高度和宽度\n",
    "            if i == 0:\n",
    "                h, w = f.size(2), f.size(3)\n",
    "            else:\n",
    "                h = min(h, f.size(2))\n",
    "                w = min(w, f.size(3))\n",
    "            \n",
    "            f_pow = torch.pow(f, 2)\n",
    "            f_mean = torch.mean(f_pow)\n",
    "            f = torch.div(f, f_mean + 1e-6)\n",
    "            global_context.append(f)\n",
    "        \n",
    "        # 在连接前调整所有特征到相同的空间维度\n",
    "        for i in range(len(global_context)):\n",
    "            # 使用自适应池化或裁剪确保所有特征具有相同的高度和宽度\n",
    "            if global_context[i].size(2) != h or global_context[i].size(3) != w:\n",
    "                # 方法1：使用自适应池化\n",
    "                global_context[i] = nn.functional.adaptive_avg_pool2d(global_context[i], (h, w))\n",
    "                \n",
    "                # 或方法2：裁剪\n",
    "                # global_context[i] = global_context[i][:, :, :h, :w]\n",
    "        \n",
    "        # 现在所有特征应该具有相同的空间维度，可以安全连接\n",
    "        x = torch.cat(global_context, 1)\n",
    "        x = self.container(x)\n",
    "        logits = torch.mean(x, dim=2)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LPRNet(\n",
      "  (backbone): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    (4): small_basic_block(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(32, 32, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
      "        (3): ReLU()\n",
      "        (4): Conv2d(32, 32, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
      "        (5): ReLU()\n",
      "        (6): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): MaxPool2d(kernel_size=3, stride=(1, 2), padding=1, dilation=1, ceil_mode=False)\n",
      "    (8): small_basic_block(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
      "        (3): ReLU()\n",
      "        (4): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
      "        (5): ReLU()\n",
      "        (6): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): small_basic_block(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
      "        (3): ReLU()\n",
      "        (4): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
      "        (5): ReLU()\n",
      "        (6): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ReLU()\n",
      "    (14): MaxPool2d(kernel_size=3, stride=(1, 2), padding=1, dilation=1, ceil_mode=False)\n",
      "    (15): Dropout(p=0.5, inplace=False)\n",
      "    (16): Conv2d(256, 256, kernel_size=(1, 4), stride=(1, 1))\n",
      "    (17): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): ReLU()\n",
      "    (19): Dropout(p=0.5, inplace=False)\n",
      "    (20): Conv2d(256, 68, kernel_size=(13, 1), stride=(1, 1))\n",
      "    (21): BatchNorm2d(68, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU()\n",
      "  )\n",
      "  (container): Sequential(\n",
      "    (0): Conv2d(516, 68, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LPRNet(lpr_max_len=8, class_num=len(CHARS), dropout_rate=0.5)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use DataLoader to load test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = LPRDataLoader(train_path.split(','), (94, 24), 8)\n",
    "val_dataset = LPRDataLoader(val_path.split(','), (94, 24), 8)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    lengths = []\n",
    "    for sample in batch:\n",
    "        img, label, length = sample\n",
    "        imgs.append(torch.from_numpy(img))\n",
    "        # imgs.append(img)\n",
    "        labels.extend(label)\n",
    "        lengths.append(length)\n",
    "    labels = np.asarray(labels).flatten().astype(np.int32)\n",
    "    return (torch.stack(imgs, 0), torch.from_numpy(labels), lengths)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using  cuda:0\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "model = LPRNet(lpr_max_len=8, class_num=len(CHARS), dropout_rate=0.5)\n",
    "epoch = 10\n",
    "\n",
    "# loss function\n",
    "ctc_loss = nn.CTCLoss(blank=len(CHARS)-1, reduction='mean')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "# optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# cpu训练还是GPU训练\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('using ', device)\n",
    "model = model.to(device)\n",
    "\n",
    "# tensorboard\n",
    "log_folder = \"./logs\"\n",
    "if not os.path.exists(log_folder):\n",
    "    os.mkdir(log_folder)\n",
    "writer = SummaryWriter(log_dir=log_folder)\n",
    "\n",
    "# 模型保存路径\n",
    "save_folder = './model'\n",
    "if not os.path.exists(save_folder):\n",
    "    os.mkdir(save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_tuple_for_ctc(T_length=18, lengths=8):\n",
    "    # T_length = lpr_max_len\n",
    "    input_lengths = []\n",
    "    target_lengths = []\n",
    "\n",
    "    for ch in lengths:\n",
    "        input_lengths.append(T_length)\n",
    "        target_lengths.append(ch)\n",
    "\n",
    "    return tuple(input_lengths), tuple(target_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(preds, pred_char=False):\n",
    "    last_chars_idx = len(CHARS) - 1\n",
    "\n",
    "    # 贪婪解码 (greedy decode)\n",
    "    pred_labels = []\n",
    "    for i in range(preds.shape[0]):\n",
    "        pred = preds[i, :, :]  # 第i张图片对应的结果，即维度为(66, 18)的二维数组\n",
    "        pred_label = []\n",
    "        for j in range(pred.shape[1]):  # 遍历每一列，找到每一列最大值的索引（index）\n",
    "            pred_label.append(np.argmax(pred[:, j], axis=0))\n",
    "        no_repeat_blank_label = []\n",
    "        pre_c = -1\n",
    "        for c in pred_label:  # 合并重复的索引值部分，删除空白标签，即为-1的值(dropout repeate label and blank label)\n",
    "            if (pre_c == c) or (c == last_chars_idx):\n",
    "                if c == last_chars_idx:\n",
    "                    pre_c = c\n",
    "                continue\n",
    "            no_repeat_blank_label.append(c)\n",
    "            pre_c = c\n",
    "        pred_labels.append(no_repeat_blank_label)\n",
    "\n",
    "    # 解码成字符串\n",
    "    if pred_char:\n",
    "        labels = []\n",
    "        for label in pred_labels:\n",
    "            lb = \"\"\n",
    "            for i in label:\n",
    "                lb += CHARS[i]\n",
    "            labels.append(lb)\n",
    "        return pred_labels, labels\n",
    "    else:\n",
    "        return pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Epoch1:-----\n",
      "Training steps:0, loss:9.965800285339355\n",
      "Training steps:100, loss:1.7906250953674316\n",
      "Accuracy on validation set: 0.0020833333333333333\n",
      "model saved\n",
      "-----Epoch2:-----\n",
      "Training steps:200, loss:0.36994829773902893\n",
      "Accuracy on validation set: 0.4947916666666667\n",
      "model saved\n",
      "-----Epoch3:-----\n",
      "Training steps:300, loss:0.16036243736743927\n",
      "Accuracy on validation set: 0.259375\n",
      "-----Epoch4:-----\n",
      "Training steps:400, loss:0.15923193097114563\n",
      "Accuracy on validation set: 0.8270833333333333\n",
      "model saved\n",
      "-----Epoch5:-----\n",
      "Training steps:500, loss:0.06860341876745224\n",
      "Accuracy on validation set: 0.7552083333333334\n",
      "-----Epoch6:-----\n",
      "Training steps:600, loss:0.09581591188907623\n",
      "Accuracy on validation set: 0.775\n",
      "-----Epoch7:-----\n",
      "Training steps:700, loss:0.06916922330856323\n",
      "Accuracy on validation set: 0.85625\n",
      "model saved\n",
      "-----Epoch8:-----\n",
      "Training steps:800, loss:0.04877069219946861\n",
      "Accuracy on validation set: 0.765625\n",
      "-----Epoch9:-----\n",
      "Training steps:900, loss:0.053539469838142395\n",
      "Accuracy on validation set: 0.8739583333333333\n",
      "model saved\n",
      "-----Epoch10:-----\n",
      "Training steps:1000, loss:0.02128470502793789\n",
      "Accuracy on validation set: 0.7177083333333333\n"
     ]
    }
   ],
   "source": [
    "# 训练和测试\n",
    "verbose_step = 100\n",
    "total_train_step = 0  # 记录训练次数\n",
    "total_val_step = 0   # 记录测试次数\n",
    "acc = 0\n",
    "for i in range(epoch):\n",
    "    print(\"-----Epoch{}:-----\".format(i+1))\n",
    "\n",
    "    # 训练\n",
    "    model.train()  # 开启训练模式\n",
    "    for images, labels, lengths in train_loader:\n",
    "        # get ctc parameters\n",
    "        input_lengths, target_lengths = sparse_tuple_for_ctc(lengths=lengths)\n",
    "        # forward\n",
    "        images = images.to(device)\n",
    "        logits = model(images)\n",
    "        log_probs = logits.permute(2, 0, 1) # for ctc loss: T x N x C\n",
    "        log_probs = log_probs.log_softmax(2).requires_grad_()\n",
    "        # backprop\n",
    "        optimizer.zero_grad()  # 优化器中的参数梯度归零\n",
    "        loss = ctc_loss(log_probs, labels, input_lengths=input_lengths, target_lengths=target_lengths)\n",
    "        if loss.item() == np.inf:  # 如果梯度爆炸\n",
    "            continue\n",
    "        loss.backward()        # 反向传播，计算梯度\n",
    "        optimizer.step()       # 优化网络\n",
    "        # 记录loss\n",
    "        if total_train_step % verbose_step == 0:  # 每verbose_step个batch，显示信息\n",
    "            print(\"Training steps:{}, loss:{}\".format(total_train_step, loss.item()))\n",
    "            writer.add_scalar(\"train_loss\", loss.item(), total_train_step)\n",
    "            writer.add_images(\"Epoch_{}\".format(epoch), images, total_train_step)\n",
    "        total_train_step += 1\n",
    "\n",
    "    # 测试\n",
    "    model.eval()  # 关闭dropout\n",
    "    total_test_loss = 0\n",
    "    total_acc_num = 0\n",
    "    Tp, Tn = 0.0, 0.0\n",
    "    with torch.no_grad():  # 关闭梯度计算\n",
    "        for images, labels, lengths in val_loader:\n",
    "            # labels: 1D -> 2D\n",
    "            targets = []\n",
    "            start = 0\n",
    "            for length in lengths:\n",
    "                label = labels[start:start+length]\n",
    "                targets.append(label.tolist())\n",
    "                start += length\n",
    "            # forward\n",
    "            images = images.to(device)\n",
    "            prebs = model(images)\n",
    "            prebs = prebs.cpu().detach().numpy()\n",
    "            # greedy decode\n",
    "            preb_labels = greedy_decode(prebs)\n",
    "            # calculate\n",
    "            for i, label in enumerate(preb_labels):\n",
    "                if len(label) != len(targets[i]):  # 长度不一致\n",
    "                    Tn += 1\n",
    "                    continue\n",
    "                if targets[i] == label:\n",
    "                    Tp += 1\n",
    "                else:\n",
    "                    Tn += 1\n",
    "        total_val_acc = float(Tp) / float(Tp + Tn)\n",
    "        print(\"Accuracy on validation set: {}\".format(total_val_acc))\n",
    "        writer.add_scalar(\"val_acc\", total_val_acc, total_val_step)\n",
    "        total_val_step += 1\n",
    "\n",
    "    # 保存模型参数\n",
    "    if total_val_acc >= acc:  # 保存最好的\n",
    "        acc = total_val_acc\n",
    "        model_path = save_folder + \"/demo.pth\"\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(\"model saved\")\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = './model/demo.pth'  # ./weights/LPR_mix.pth ./weights/LPR_demo.pth\n",
    "# # 使用map_location，防止GPU训练的模型导入到CPU上测试会出错\n",
    "# dic = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "# model.load_state_dict(dic)\n",
    "# # file_path = '../chinese_license_plate_generator/train/藏00S8DQV_green_car_False.jpg'\n",
    "# file_path = './dataset/test/皖A0C757.jpg'\n",
    "# image = cv2.imread(file_path)\n",
    "# height, width, _ = image.shape\n",
    "# if width != 94 or height != 24:\n",
    "#     image = cv2.resize(image, (94, 24))  # 缩放\n",
    "# # 查看图片\n",
    "# img = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "# img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAYAF4DASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwCa30/TmjVorTUnXP3t/GaswadBeTGK3026eUdUeXp+dbWhvMHuo5JpEhDfJsIwuenWpSlxZeK7S5nuGmhnGwOMbsHjBFelPDYfnaUehyyxWJjFPn3MZ9Ia2t5pZ9IdBGCQVmLAj86rWlwl3IsEekxmQ9N0hJq7qyaPa3VxbsL0T5IDbgVzVTRru3tb61lVpPNR8SAn5cV1UMDQjTlNxuc88bVckuc1G0y9hR3i0u0ZlGWSNwWA+nWn2dpcXlqZxFZQoP4pOBmrMEctl4zD7HEEx3D5eCp9+9RQaLJca5c2mW8hXLEA8DmsFSpK6sjZ1qkle7D7HqRvYrR47SMyrmKQRhlb8qybu71G0uZLdjGHjbYTsxk10V5qgh1OyigieOzt3wGKcE9+fSq3iTSLmTWXmtoHmSZQ6soJ61tQjT50qkVZo5q1ScoXpyd0yW0sprrSRfNqL2yr8sirFvBP9KjR411CC3j1aSVJWwx8vaV/Ok0WfU7TzrKEQpN97yZxgt7jNaN5C76K1zqMEVvfxHKyKAprJ01Rk4rVPY2lOU4q3TcbfW9rp8rwz396JAuQ6wcf/Xrm9Yto9QsYyw8zbJwWU88HnHrXU29zqdy9tLFqEN1aDhwFAP0OagvBZLqE0duieXwxAbIDHrXPWTdJwauzSjL95zLQ5mwupL22h1HT7y1AnhG4ucgH2p12t5PPbSyX9qpgxs8s55HeiiojjakdkvuFVoRdkXrnV5mTc2q6YknGfMt1Yn681mb9MaYzXWsWnmNy2wKo/AZooohjKsU7fkW8PTktUWR4k023gEcPimJV6Ab1OPpnpVODxNo9lLJKPFeJD95lcZNFFT9ZmV9XgFx420CUFJ/FTOGHILj+lQ/8J/4YiVQNfmwo2gK78Ciil9ZqLZj9hB9CjeeN/B1yCZdTunlByHXeSfxxUR+IPhJUCNdXcgx/GrN/Oiioli6zfxCVCmtkQj4keEoukNw4B6eWaa3xT8NIcQ2dynrtTGf1ooodeq95M0jSppbH/9k=",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=94x24>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 24, 94])\n",
      "['皖闽E0L']\n"
     ]
    }
   ],
   "source": [
    "path = './model/demo.pth'\n",
    "# 使用map_location，防止GPU训练的模型导入到CPU上测试会出错\n",
    "dic = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "model.load_state_dict(dic)\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "file_path = './dataset/test/苏E02L55.jpg'\n",
    "# 读取图像为NumPy数组\n",
    "numpy_image = cv2.imread(file_path)\n",
    "height, width, _ = numpy_image.shape\n",
    "if width != 94 or height != 24:\n",
    "    numpy_image = cv2.resize(numpy_image, (94, 24))  # 缩放\n",
    "\n",
    "# 查看图片\n",
    "img = Image.fromarray(cv2.cvtColor(numpy_image, cv2.COLOR_BGR2RGB))\n",
    "display(img)\n",
    "\n",
    "# 预处理图像 - 使用NumPy操作\n",
    "numpy_image = numpy_image.astype('float32')\n",
    "# (height, width, channel) -> (channel, height, width)\n",
    "numpy_image = np.transpose(numpy_image, (2, 0, 1))\n",
    "# 归一化\n",
    "numpy_image = numpy_image / 255.0\n",
    "# 改变shape\n",
    "numpy_image = numpy_image.reshape(1, 3, 24, 94)\n",
    "\n",
    "# 转换为PyTorch张量\n",
    "tensor_image = torch.from_numpy(numpy_image)\n",
    "print(tensor_image.shape)  # 应该输出 torch.Size([1, 3, 24, 94])\n",
    "\n",
    "# 现在可以将张量传递给模型进行推理\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(tensor_image)\n",
    "    output = output.cpu().detach().numpy()\n",
    "    labels_idx, labels = greedy_decode(output, True)\n",
    "    print(labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee567",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
